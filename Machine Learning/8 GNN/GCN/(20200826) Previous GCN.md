# GCN 网络 (1)

## 主要内容

GCN 网络 (前身)

## 新名词

谱方法（spectral method）；空间方法（spectral method）；图的拉布拉斯矩阵（graph laplacian）

## 我的关注点

1. 图的定义：
   $$
   G=(V,E,W)
   $$

   - 这边 $$V$$ 是**结点集**，并令 $$n=|V|$$ ； $E$ 是**边集**； $$W$$ 是图的**权重邻接矩阵**，有 $$W \in \mathbb{R}^{n \times n}  $$

   - 每个结点都有 $$d$$ 个**特征**（即，结点 $$i$$ 的**特征向量** $$f_i$$ 是一个 **$$d$$ 维的向量**）

   - 所有结点的**特征向量**汇聚成一个**特征矩阵**： $$X \in \mathbb{R} ^ {n \times d}$$ ，**特征矩阵**的每一个**列**可称之为图的所有结点上的一个「**信号**」(signal)

   - 例如某个**特征矩阵**如下：
     $$
     X=\begin{bmatrix} 
     0 & 1 & 2 & 3 & 5 \\
     3 & 7 & 1 & 4 & 0 \\
     4 & 1 & 8 & 5 & 7 \\
     9 & 0 & 6 & 9 & 7 \\
     \end{bmatrix}
     =
     \begin{bmatrix} 
     f_1^T \\
     f_2^T \\
     f_3^T \\
     f_4^T \\
     \end{bmatrix}
     =
     \begin{bmatrix} 
     s_1, s_2, s_3, s_4, s_5
     \end{bmatrix}
     $$
     上边 $$f_1$$ 至 $$f_4$$ 是这四个神经元的**特征向量**（5 维），而 $$s_1$$ 至 $$s_5$$ 是图节点的 5 种**信号**，其中： $$f_i \in \mathbb{R} ^ d $$ ，而 $$s_i \in \mathbb{R} ^ n$$ 。

2. **拉布拉斯矩阵**定义了图的「导数」，刻画了信号在图上的平滑程度
   $$
   L=D-W
   $$
   - 其中， $$D$$ 是图**节点的度矩阵**，可以透过以下算式算出：
     $$
     D_{ii}=\sum_{j}W_{ij}
     $$
     注意 $$D$$ 是对角矩阵（每个节点的度分布在主对角线上）
   - 标准化拉布拉斯矩阵的定义是：
     $$
     L=I-D^{-\frac{1}{2}}WD^{-\frac{1}{2}}
     $$
     其中 $$I$$ 叫做「身份矩阵（Identical Matrix）」？
   
3. 谱方法：将图变换到「谱域」，在谱域上做卷积运算后再变换回去

4. 图的**傅立叶变换**（Graph Fourier transform）：对于图上的一个信号： $$x \in \mathbb{R} ^ n$$
   $$
   \begin{aligned}
   \hat x &= U^Tx &(1)\\
   x &= U \hat x &(2)
   \end{aligned}
   $$
   其中，(1) 是将信号变换到谱域的算式，(2) 是将信号从谱域变换回来的算式， $$U$$ 是？

5. 卷积定理（傅立叶变换后）：
   $$
   \begin{aligned}
   x *_G y &= U((U^Tx)\odot(U^Ty)) &(1)\\
   x *_G y &= U(g_\theta U^Tx) &(2)\\
   \end{aligned}
   $$
   其中， $$x$$ 是一个输入信号，而 $$y$$ 是另一个信号（卷积核）， $$*_G$$ 是图卷积符号， $$U$$ 依然不知道是啥。 $$U^Ty$$ 就是这个图在谱域的卷积核。

   上面 (1) 式可以写成 (2) 式，该式分三步走：

   - 第一步，将节点域的信号变换到谱域—— $$U^Tx$$ ；
   - 第二步，在谱域做卷积—— $$g_\theta U^Tx$$ ，其中 $$g_\theta = {\rm diag}((U^Ty)^T)$$  ，才是真正的卷积核；
   - 第三步，将卷积结果逆变换回去—— $$U(g_\theta U^Tx)$$

## 派生问题

1. 上边的东西已经过时了吧？

