# CNN 网络

## 主要内容

CNN 网络 (基本)

## 新名词

特征、过拟合、欠拟合，卷积核（filter），特征图（feature_map）

## 我的关注点

1. 特征 (feature)：特征数目过少，就无法精确进行分类（欠拟合）；特征数目过多，就导致过于注重某个特征导致分类错误（过拟合）

2. 用神经网络的原因

   - **特征提取比较高效**——神经网络不用人提前做大量的数据准备工作，不用去手动设计特征的样貌及设计特征的数量，这些东西都由机器自己学习得到。只需将数据喂给神经网络，它便可自己学习。
   - **数据提取比较简易**——神经网络不必对数据进行过多处理，比如对输入数据的量纲统一化。
   - **参数数目可以自动学习得到**——非神经网络解决分类问题时遇到的参数需要研究人员对内部理论知识比较精通才能正确选择。但是神经网络可以透过不断修正参数而调节到**最优质**的参数（成为收敛），让模型的误差比较小。

3. 神经网络的缺点

   - 参数比较多，所需的参数随着网络层数增加而增加的很恐怖
   - 参数比较多的复杂模型，参数的调整会更难一些，容易导致过拟合
   - 梯度消失：后向传播 (BP) 的过程中调参会令权值的梯度减小，一旦梯度减小到0，权值就无法更新，神经元就「死」了。后果是收敛会变难。因此，**图像领域一般不用基本（三层）神经网络**

4. CNN神经网络

   - 弥补基本神经网络在**图像领域**的劣势。基本神经网络是「全连接」的，会造成边的数量很多，进而造成w和b的总数量很多。为了识别一张图像，可能需要十几万个参数。计算及存储成本高
   - 图片具有「局部特性」，不必关注每一个特征也能进行分类

5. 同一层的神经元可以共享卷积核，这样对于高位数据的处理将会变得非常简单。

6. 每一层的卷积核大小和个数可以**人为定义**，但一般情况下，根据实验得到的经验来看，会在越靠近输入层的卷积层设定少量的卷积核，越往后，卷积层设定的卷积核数目一般就越多。

7. 通常来说，池化（下/降采样）方法一般有一下两种：

   - MaxPooling：取滑动窗口里最大的值
   - AveragePooling：取滑动窗口内所有值的平均值

8. MaxPooling：

   进行MaxPooling操作后，**一般而言**，提取出的是真正能够识别特征的数值，其余被舍弃的数值，对于我提取特定的特征并没有特别大的帮助。

   但是，**并非所有情况MaxPooling的效果都很好**，有时候有些周边信息也会对某个特定特征的识别产生一定效果。

   有时，不如把卷积后不加Max Pooling的结果与卷积后加了Max Pooling的结果输出对比一下，看看Max Pooling是否对卷积核提取特征起了**反效果**。

9. 卷积、池化均会导致图片变小，可能最后会变成0尺寸的图片，后边添加神经网络层时，图片就被缩没了，因此需要**填0（Zero Padding）**。

   Zero Padding是在图片周围填上0，令图片变大。

   **我们通常希望图片做完卷积操作后保持图片大小不变**。所以我们一般会选择尺寸为3x3的卷积核及1的zero padding，或者5x5的卷积核及2的zero padding，这样计算后，可以保留图片的原始尺寸。

10. 如果想要叠加层数，一般是叠加“Conv-MaxPooing"。不断的设计卷积核的尺寸、数量，提取更多的特征，最后识别不同类别的物体。

11. 做完最后一次Max Pooling后，我们就会把这些数据“拍平”（丢到Flatten层），然后把Flatten层的output放到**full connected Layer里，采用softmax等方法对其进行分类**。

    所谓“拍平”，就是把多维矩阵变成一个1维数组，方便之后做全联接。

    全连接层到输出层就是正常的神经元与神经元之间的邻接相连，通过softmax函数计算后输出到output，得到不同类别的概率值，**输出概率值最大的即为该图片的类别**。

## 派生问题

1. 卷积神经网络的卷积核大小如何确定？

   卷积核大小必须大于1才有提升感受野的作用，1排除了。

   而大小为偶数的卷积核即使对称地加padding也不能保证输入feature map尺寸和输出feature map尺寸不变（画个图算一下就可以发现），2排除了。

   所以一般都用3作为卷积核大小。

   但是，数字越大的卷积核特征提取的效果便越好，但是计算量会变大许多。不必要都选3作为卷积核大小，也可以根据计算水平选择。

2. 卷积核数目如何确定？

   靠猜。一般情况下，多尝试（「调参」）或攫取其他前人的经验以确定第一次测试用的。
