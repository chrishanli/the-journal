# **BP神经网络算法推导及代码实现笔记**

LH25/4/2020  ·  整理自微信公众号

## ▌一、前言：

本文货很干，堪比沙哈拉大沙漠，自己挑的文章，含着泪也要读完！

## ▌二、科普：

- 生物上的神经元就是接收四面八方的刺激（输入），然后做出反应（输出），给它一点☀️就灿烂。
- 仿生嘛，于是喜欢放飞自我的 某些人 就提出了人工神经网络。一切的基础-->人工神经单元，看图：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHtQicSDAibNolFJGWNL1bTQib2YHWMVaZjBrP3xRj3Fg606FSU75XG1gNg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## ▌三、通往沙漠的入口：神经元是什么？有什么用？

开始前，需要搞清楚一个很重要的问题：人工神经网络里的神经元是什么，有什么用。只有弄清楚这个问题，你才知道你在哪里，在做什么，要往哪里去。

首先，回顾一下神经元的结构，看下图, 我们先忽略激活函数不管：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHYszo9nk6zSrIRL4TQKh1sVO3sz59ns1jIs3KYTcqxJFUtcgCPCzdMw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

该神经元的输入分别是：
$$
x_1, x_2, …, x_n\qquad(3.1)
$$
输出是：
$$
y\qquad(3.2)
$$
上述两者的关系是（加权后相加）：
$$
\begin{align}
y&=[w_1, w_2, ..., w_n]\cdot [x_1, x_2, ..., x_n]^T + b\\
&=\sum_{i=1}^{n}w_ix_i+b\qquad(3.3)
\end{align}
$$
其中，$w_i\ (i = 1, 2, ..., n)$为每条神经元间连接线上的**权重**，稍后会介绍。

没错，开始晒公式了！我们的数据都是离散的，为了看得更清楚点，所以换个表达方式，把离散的数据写成向量。该不会忘了向量是啥吧？回头致电问候一下当年的体育老师！

将输入改写成列向量：
$$
x = [x_1,x_2,...,x_n]^T\ \ (3.4)
$$
将权重改写成行向量：
$$
w = [w_1,w_2,...,w_n]\ \ (3.5)
$$
由于3.5是$1\times n$矩阵而3.4是$n\times 1$矩阵，则矩阵$w\cdot x$会是$1\times 1$矩阵，即一个数。于是，3.3式还可以写为：
$$
\begin{align}
y&=\sum_{i=1}^{n}w_ix_i+b\\
&=wx+b\ \ (3.6)
\end{align}
$$
上面$wx+b$的$w$完全可以看成是该神经元对应的输入直线的斜率。

现在回答问题刚才的问题：

- 一个神经元是什么：参照式（3.6），从函数图像角度看，这就是一根直线。

- 一个神经元有什么用：要说明用途就要给出一个应用场景：**分类**。**一个神经元就是一条直线，相当于楚河汉界，可以把红棋绿棋分隔开，此时它就是个分类器**。

  所以，在**线性场景下**，**单个神经元能达到分类的作用**，它总能学习到一条合适的直线，将两类元素区分出来。这里强调是线性场景。

先睹为快，看效果图，自己可以去玩：传送门

http://t.cn/RBCoWof

![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHk6hM973D9eNgibaolIUlCJUW1qvzyPOyibskTIcXvUWUVOs72zBNIZVQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

对上面的图简单说明一下:

- $(x_1,x_2)$对于神经元的输入而言都是一个 $x$，而对我们而言，这数据就是意义上的点的坐标，我们习惯写成$(x,y)$。
- 有两类点需要被分类（红色和蓝色）

又要划重点了：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHwGvESueu8tKYj0WsmIxBpEx8iaHabhNLtX8Y369E4XU8sKAsoZXOqYA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

我们需要对神经元的输出做判定，那么就需要有**判定规则**，通过判定规则后我们才能拿到我们想要的结果。我们规定这个规则是：

1. 假设，0代表红点，1代表蓝点（这些数据都是事先标定好的，在监督学习下，神经元会知道点是什么颜色并以这个已知结果作为标杆进行学习）
2. **当神经元输出小于等于 0 时，最终结果输出为 0**，这是个红点
3. **当神经元输出大于 0 时，最终结果输出为 1**，这是个蓝点

上面提到的规则让我闻到了***激活函数***的味道！（这里只是线性场景，虽然不合适，但是简单起见，使用了单位阶跃函数来描述激活函数的功能）

单位阶跃函数的定义是：当$x\leq0$时，$y=0$; 当$x>0$时，$y=1$。这是这个阶跃函数的长相：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHc6P84PJmJSiba9lyS5Yevt3dH89Nmgb3WXns4w8dGoMBeMG9PEysxVA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

应用了该函数作为**激活函数**的神经元的长相：

 ![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHYBJ4evNAyHT9mFfDBF0bK97dzrPciboWZP3ow4ZSiaU7Eqev5HC4sQ5g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## ▌四、茫茫大漠第一步：激活函数是什么，有什么用？

从上面的例子，其实已经说明了激活函数的作用；但是，我们通常面临的问题，一旦都不会是简单的线性问题，不能用单位阶跃函数作为激活函数，原因是：

> 阶跃函数在$x=0$时**不连续，即不可导**，而在非0处导数为0。用人话说就是，它具备输出限定在[0,1]，但是它不具备丝滑的特性，这个特性很重要。并且在非0处导数为0，也就是硬饱和，压根儿就没**梯度**可言，梯度也很重要，梯度意味着在神经元传播间是有反应的，而不是“死”了的。

接下来说明下，一般激活函数所具备的特性有什么，只挑重要的几点特性讲：

- **非线性：** 即导数不是常数，不然就退化成直线了。对于一些画一条直线仍然无法分开的问题，非线性可以把直线掰弯，自从变弯以后，就能包罗万象了。

- **几乎处处可导**：也就是具备“丝滑的特性”，不要应激过度，要做正常人。数学上，**处处可导**为后面说到的后向传播算法（BP算法）提供了核心条件

- **输出范围有限：**一般是限定在[0,1]，有限的输出范围使得神经元对于一些比较大的输入会比较稳定。

- **非饱和性**：饱和就是指，当输入比较大的时候，输出几乎没变化了，那么会导致梯度消失！什么是梯度消失：就是你天天给女生送花，一开始妹纸还惊喜，到后来直接麻木没反应了。梯度消失带来的负面影响就是会限制了神经网络表达能力，词穷的感觉你有过么。

  sigmoid，tanh函数都是软饱和的，阶跃函数是硬饱和。**软**是指输入趋于无穷大的时候输出无限接近上线，**硬**是指像阶跃函数那样，输入非0输出就已经始终都是上限值。数学表示就懒得写了，传送门在此（https://www.cnblogs.com/rgvb178/p/6055213.html），里面有写到。

  如果激活函数是饱和的，带来的缺陷就是系统迭代更新变慢，系统收敛就慢，当然这是可以有办法弥补的，一种方法是使用交叉熵函数作为损失函数，这里不多说。ReLU是非饱和的，亲测效果挺不错，所以这货最近挺火的。

- **单调性**：即导数符号不变。输出的y要么一直大于0，要么一直小于0，不要上蹿下跳。导数符号不变，让神经网络训练容易收敛。

这里只说我们用到的激活函数（sigmoid函数）：
$$
y=\frac{1}{e^{-x}+1}\ \ (4.1)
$$
该函数的图像是：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHibVXB31bkHW5YOWDQFeD2Qk2ctHHaBvq5iafcRRASicVJPEiadicvGhSPaw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

接下来先求一下它的导数，作为推导BP算法的手续。先先祭出大杀器，复习一下复合函数求导法则：
$$
\begin{align}
&法则一：\qquad[u(x)\pm v(x)]'=u(x)'\pm v(x)'\\
&法则二：\qquad[u(x)\cdot v(x)]'=u'(x)\cdot v(x)+v'(x)\cdot u(x)\\
&法则三：\qquad(\frac{u(x)}{v(x)})'=\frac{u'(x)\cdot v(x)-u(x)\cdot v'(x)}{v^2(x)}
\end{align}
$$
计算sigmoid函数的导数：
$$
\begin{align}
y'&=(\frac{1}{e^{-x}+1})'\\
\end{align}
$$
令$u = 1$及$v = e^{-x}+1$得：
$$
\begin{align}
y'&=(\frac{u}{v})'\\
&=\frac{u'v-uv'}{v^2}\\
&=\frac{0\cdot(e^{-x}+1)-1\cdot(-e^{-x}+0)}{(e^{-x}+1)^2}\\
&=\frac{e^{-x}}{(e^{-x}+1)^2}\\
&=\frac{1}{e^{-x}+1}\cdot\frac{e^{-x}}{e^{-x}+1}\\
&想办法凑出y，得：\\
y'&=\frac{1}{e^{-x}+1}\cdot\frac{1+e^{-x}-1}{e^{-x}+1}\\
&=\frac{1}{e^{-x}+1}\cdot(1-\frac{1}{e^{-x}+1})\\
\end{align}
$$
注意到式(4.1)，即$y=\frac{1}{e^{-x}+1}$，故有sigmoid函数$y$的导数：
$$
y'=y\cdot(1-y)\qquad(4.5)
$$
由于我们有电脑，不用像高考那样手画也能即刻得出它的导数图像：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHZ51IwTfuA3rPtPLdFOQnjYYviaBFZ0sPIoqlUs6PwKvxw0SXkib4jZ2Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## ▌五、沙漠中心的风暴：BP(Back Propagation)算法

### **1. 神经网络的结构**

经过上面的介绍，单个神经元不足以让人心动，唯有组成网络。

神经网络是一种分层结构，一般由**输入层**，**隐藏层**（hidden layer）及**输出层**组成。所以神经网络至少有3层。而隐藏层大于1，总层数大于3的就是我们所说的深度学习了。

各层的功能是：

- 输入层：接收原始数据，然后往隐层送
- 输出层：神经网络的决策后输出
- 隐藏层：该层可以说是神经网络的关键，相当于对数据做一次特征提取。隐藏层的意义，**是把前一层的向量$\boldsymbol w$和向量$\boldsymbol x$变成新的向量**。其实就是坐标变换，说人话就是把数据做平移，旋转，伸缩，扭曲，让数据变得**线性可分**。可能这个不那么好理解，举个栗子：

下面的图左侧是原始数据，中间很多绿点，外围是很多红点，如果你是神经网络，你会怎么做呢？

![image-20200425090146300](/Users/phantef/Library/Application Support/typora-user-images/image-20200425090146300.png)

一种做法：把左图的平面看成一块布，把它缝合成一个闭合的包包（相当于数据变换到了一个3维坐标空间，如右图），然后把有绿色点的部分撸到底部（伸缩和扭曲），然后外围的红色点自然在另一端了，要是姿势还不够帅，就挪挪位置（平移）。这时候干脆利落的砍一刀，绿点红点就彻底区分开了。

**重要的东西再说一遍：神经网络换着坐标空间玩数据，根据需要，可降维，可升维，可大，可小，可圆可扁，就是这么“无敌”**

这个也可以自己去玩玩，直观的感受一下：传送门

https://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html

观察动态生成过程食用更佳哦

### **2. 正反向传播过程**

看图，这是一个典型的三层神经网络结构，第一层是输入层，第二层是隐藏层，第三层是输出层。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHgib4FxqibfmRR0Dyxc3KP69xm81hmNBy71Y5X0ibalxpnicd4KWec57Ctg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

PS：不同的应用场景，神经网络的结构要有针对性的设计，这里仅仅是为了推导算法和计算方便才采用这个简单的结构。

我们以战士打靶，目标是训练战士能命中靶心成为神枪手作为场景：
我们手里有这样一些数据：一堆该战士持枪时*枪*的摆放位置$(x,y)$，以及射击结果（命中靶心和不命中靶心）。
我们的目标是：训练出一个神经网络模型，输入一个点的坐标（射击姿势），它就告诉你这个点是什么结果（是否命中）。
我们的方法是：训练一个能根据误差不断自我调整的模型，训练该模型的步骤是：

- 第一步，正向传播（FP）：把点的坐标数据输入神经网络，然后开始一层一层的传播下去，直到输出层输出结果。
- 第二步，反向传播（BP）：就好比一位大兵去靶场打靶，枪的摆放位置（输入），和靶心（期望的输出）是已知的。这位大兵（神经网络）一开始的时候是这样做的：随便开一枪（$w$，$b$参数初始化称随机值），观察结果（这时候相当于进行了一次正向传播）。然后发现，偏离靶心左边，应该往右点儿打。所以这位大兵开始根据偏离靶心的距离（误差，也称损失）调整了射击方向往右一点（即调整$w$，$b$的值。这时，完成了一次反向传播）
- 当完成了一次正反向传播，也就完成了一次神经网络的训练迭代。反复调整射击角度（反复迭代），误差越来越小，大兵打得越来越准，神枪手模型也就诞生了。 

### **3. BP算法推导及计算**

#### （1）步骤1：正向传播（FP）

* 先进行参数初始化，为后面的操作提供数据。

  其中，例如输入的两个值是：
  $$
  i_1=0.1, i_2=0.2
  $$
  期望该网络能够从上面这两个值分别输出这两个值的对应结果（0或1，取相近的一个小数就行），例如：
  $$
  o_1=0.01, o_2=0.99
  $$
  权重$w$及偏置$b$都是随机的，例如初始的时候初始化了下列一组权重及偏置：
  $$
  \begin{align}
  &w_1=0.1, w_2=0.2, w_3=0.3, w_4=0.4\\
  &w_5=0.5, w_6=0.6, w_7=0.7, w_8=0.8\\
  &b_1=0.55, b_2=0.56, b_3=0.66, b_4=0.67
  \end{align}
  $$
  这些值对应上面的典型网络图中的值，也就是下图中的值：

  ![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHgib4FxqibfmRR0Dyxc3KP69xm81hmNBy71Y5X0ibalxpnicd4KWec57Ctg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

  可以注意到：$w$的个数和网络中“神经纤维”的条数相等，而$b$的个数和网络中神经元的个数相等。

* 进行正向传播：

  首先计算隐藏层神经元$h_1$的输入加权和，并算出数字：
  $$
  \begin{align}
  {\rm in}_{h_1}&=w_1i_1+w_2i_2+1\cdot b_1\qquad(5.1)\\
  &=0.1\times 0.1+0.2\times 0.2+1\times 0.55\\
  &=0.6
  \end{align}
  $$
  然后计算出该隐藏层神经元$h_1$的输出，需要透过其激活函数（我们都取激活函数为Sigmoid函数）求出。然后算出数字：
  $$
  \begin{align}
  {\rm out}_{h_1}&=\frac{1}{e^{-{\rm in}_{h_1}}+1}\qquad(5.2)\\
  &=\frac{1}{e^{-0.6}+1}\\
  &=0.6456563062
  \end{align}
  $$
  同理，由算式(5.2)算出神经元$h_2$的输出：
  $$
  {\rm out}_{h_2}=0.6592603884
  $$

* 将隐藏层的结论-->输出层：

  首先计算输出层神经元$O_1$的输入加权和：
  $$
  \begin{align}
  {\rm in}_{O_1}&=w_5\cdot {\rm out}_{h_1}+w_6\cdot {\rm out}_{h_2}+1\cdot b_3\qquad(5.3)\\
  &=0.5\times 0.6456563062+0.6\times 0.6592603884+1\times 0.66\\
  &=1.3783843861
  \end{align}
  $$
  接着计算该隐层神经元$O_1$的输出。该神经元也用Sigmoid函数作为激活函数：
  $$
  \begin{align}
  {\rm out}_{O_1}&=\frac{1}{e^{-{\rm in}_{O_1}}+1}\qquad(5.4)\\
  &=\frac{1}{e^{-1.3783843861}+1}\\
  &=0.7987314002
  \end{align}
  $$
  同理，由算式(5.4)算出神经元$O_2$的输出：
  $$
  {\rm out}_{O_2}=0.8374488853
  $$

至此，正向传播的过程结束，输出层的输出结果是：
$$
[0.7987314002, 0.8374488853]
$$
但是我们希望它能输出$[o_1, o_2]$，即
$$
[0.01, 0.99]
$$
所以明显的差太远了！，这个时候我们就需要利用***反向传播***，更新权值$w$及$b$，然后重新计算输出。

#### （2） 步骤2：反向传播：

要计算进行反向传播，首先要让求出“输出值”和“期望值”的误差，好让隐藏层、输入层的参数调整操作有所依据。

##### 	a) 计算总输出误差

这里，我们利用均方误差来表示输出值和期望值的误差（损失函数），即：
$$
E_{{\rm out}_{O_i}}=\frac{1}{l}({\rm expected_{{\rm out}_{O_i}}}-{\rm out}_{O_i})^2\qquad(i=1, 2,...,m)
$$
在本例中，由于只有2个输入数据，因此$m=2$。我们还取$l=2$，为了后边求导能把分数约掉。即
$$
\begin{align}
E_{{\rm out}_{O_i}}&=\frac{1}{2}({\rm expected_{{\rm out}_{O_i}}}-{\rm out}_{O_i})^2\\
&=\frac{1}{2}(o_i-{\rm out}_{O_i})^2\qquad(5.5.1)
\end{align}
$$
注意这些算式中字母“o”的大小写！大写的O代表输出层神经元，而小写的o代表期望该输出层神经元输出的值。故有输出总误差：
$$
\begin{align}
E_总&=\sum_{i=1}^{2}E_{{\rm out}_{O_i}}\qquad(5.5)\\
&=E_{{\rm out}_{O_1}}+E_{{\rm out}_{O_2}}\\
&=\frac{1}{2}(o_1-{\rm out}_{O_1})^2+\frac{1}{2}(o_2-{\rm out}_{O_2})^2\\
&=\frac{1}{2}(0.01-0.7987314002)^2+\frac{1}{2}(0.99-0.8374488853)^2\\
&=0.0116359213+0.3110486109\\
&=0.3226845322
\end{align}
$$
其中也可得到
$$
E_{{\rm out}_{O_1}}=0.0116359213\\
E_{{\rm out}_{O_2}}=0.3110486109
$$
PS：用均方误差作为误差的计算，因为它简单，实际上用的时候效果不咋滴。如果激活函数是饱和的，带来的缺陷就是系统迭代更新变慢（如果一开始值就在阈值上下波动，计算次数可能会很多），系统收敛就慢，当然这是可以有办法弥补的，一种方法是使用***交叉熵函数***作为损失函数。

交叉熵做为代价函数能达到上面说的优化系统收敛下欧工，是因为它在计算误差对输入的梯度时，抵消掉了激活函数的导数项，从而避免了因为激活函数的“饱和性”给系统带来的负面影响。

如果项了解更详细的证明可以点 --> 传送门（https://blog.csdn.net/lanchunhui/article/details/50086025）

下面就是用***交叉熵函数***表达的损失函数：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHaV6bEWT36PutBOFctTRlsQ9ldhXwzQQJ2GzNg0hboBAocLwibddLdfg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

它对输出的偏导数可以表示为：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHJnUXpnyQKGxmkKGFy8ro0ibhGlAqNdznFwLWEl0mQPFUHKY3DjYYvQg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

本例中暂时不用交叉熵函数。

##### b) 隐藏层<-->输出层的权值w及偏置b的更新

- 先回忆下《高等数学》学习的链式求导法则：

  假设y是u的函数，而u是x的函数，即
  $$
  y=f(u), u=f(g)
  $$
  其对应的复合函数是
  $$
  y=f(g(x))
  $$
  则该复合函数y对x的导数即是
  $$
  \frac{{\rm d}y}{{\rm d}x}=
  \frac{{\rm d}y}{{\rm d}u}\cdot
  \frac{{\rm d}u}{{\rm d}x}
  $$

- 以更新$w_5$举例：

  我们知道，权重$w_i$的大小能直接影响输出，$w_i$不合适那么会使得输出具有误差。要想直到某一个$w_i$值对误差影响的程度，可以用误差对该$w_i$的***变化率***来表达。如果$w_i$的一点点变动，就会导致误差增大很多，说明这个$w_i$对误差影响的程度就更大，或称误差对该$w_i$的变化率越高。

  而误差对$w_i$的变化率，可以利用“**误差对$w_i$的偏导**”来表达。

  因此，在下图中，总误差的大小首先受输出层神经元$O_1$的输出（${\rm out}_{O_1}$）影响；继续反推，$O_1$的输出受它自己的输入${\rm in}_{O_1}$的影响，而它自己的输入会受到$w_5$及$w_6$的影响。这就是连锁反应，从结果找根因。
  
  ![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHlC2gbVx4Uc7FJkiaTD1fOwaQRdg9xFzXADnbzazKJffNVerjIbocpJA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
  
  那么，根据链式法则有：
  $$
  \frac{\partial E_总}{\partial w_5} = 
  \frac{\partial E_总}{\partial {\rm out}_{O_1}}\cdot
  \frac{\partial {\rm out}_{O_1}}{\partial {\rm in}_{O_1}}\cdot
  \frac{\partial {\rm in}_{O_1}}{\partial w_5}
  \qquad(5.6)
  $$
  现在对式(5.6)的各项挨个计算：
  
  **i) 第一项：由式(5.5)得：**
  $$
  E_总=\frac{1}{2}(o_1-{\rm out}_{O_1})^2+\frac{1}{2}(o_2-{\rm out}_{O_2})^2\\
  $$
  ​	则：
  $$
  \begin{align}
  \frac{\partial E_总}{\partial {\rm out}_{O_1}}&=
  \frac{\partial (\frac{1}{2}(o_1-{\rm out}_{O_1})^2+\frac{1}{2}(o_2-{\rm out}_{O_2})^2)}{\partial {\rm out}_{O_1}}\qquad(5.7)\\
  &=2\cdot\frac{1}{2}(o_1-{\rm out}_{O_1})^2\cdot (0-1)+0\\
  &=-(o_1-{\rm out}_{O_1})\\
  &=-(0.01-0.7987314002)\\
  &=0.7887314002
  \end{align}
  $$
  **ii) 第二项：${\rm out}_{O_1}$和${\rm in}_{O_1}$关系就是激活函数的关系。由激活函数表达式(4.5)得：**
  $$
  \begin{align}
  {\rm out}_{O_2}&=\frac{1}{e^{-{\rm in}_{O_1}}+1}\qquad(5.8)
  \\
  \frac{\partial {\rm out}_{O_1}}{\partial {\rm in}_{O_1}}&=
  \frac{\partial (\frac{1}{e^{-{\rm in}_{O_1}}+1})}{\partial {\rm in}_{O_1}}\\
  &={\rm out}_{O_1}(1-{\rm out}_{O_1})\qquad(5.9)\\
  &=0.7987314002\cdot(1-0.7987314002)\\
  &=0.1607595505
  \end{align}
  $$
  **iii) 第三项：由式5.3得**
  $$
  {\rm in}_{O_1}=w_5\cdot {\rm out}_{h_1}+w_6\cdot {\rm out}_{h_2}+1\cdot b_3\qquad(5.10)
  $$
  ​	故：
  $$
  \begin{align}
  \frac{\partial {\rm in}_{O_1}}{\partial w_5}&=
  \frac{\partial (w_5\cdot {\rm out}_{h_1}+w_6\cdot {\rm out}_{h_2}+1\cdot b_3)}{\partial w_5}\qquad(5.11)\\
  &=1\cdot w_5^{1-1}\cdot {\rm out}_{h_1}+0+0\\
  &={\rm out}_{h_1}\\
  &=0.6456563062
  \end{align}
  $$
  **iv) 综上求和，得(5.6)的归纳式：**
  $$
  \begin{align}
  \frac{\partial E_总}{\partial w_5} &=
  \frac{\partial E_总}{\partial {\rm out}_{O_1}}\cdot
  \frac{\partial {\rm out}_{O_1}}{\partial {\rm in}_{O_1}}\cdot
  \frac{\partial {\rm in}_{O_1}}{\partial w_5}&(5.12)\\
  
  &=-(o_1-{\rm out}_{O_1})\cdot {\rm out}_{O_1}\cdot(1-{\rm out}_{O_1})\cdot{\rm out}_{h_1}&(5.13)\\
  &=\sigma_{O_1}\cdot{\rm out}_{h_1}\\
  &=0.0818667051
  \end{align}
  $$
  其中，$\sigma_{O_1}$是只跟$O_1$有关的变量，可写作
  $$
  \sigma_{O_1}=-(o_1-{\rm out}_{O_1})\cdot {\rm out}_{O_1}\cdot(1-{\rm out}_{O_1})\qquad(5.14)
  $$
  
- 根据求$E_总$和$w_5$偏导数，可以类似地求出$E_总$和$w_6$的偏导数、$E_总$和$b_3$的偏导数。例如可以求出$E_总$和$b_3$的偏导数为：
  $$
  \begin{align}
  \frac{\partial E_总}{\partial w_3}=\sigma_{O_1}\qquad(5.15)
  \end{align}
  $$
  其中$\sigma_{O_1}$的定义和(5.14)一致。

  有个叫做***学习率***的东西，目前我们学习率暂时取0.5（一般取0.1左右的值为佳）。关于学习率，不能过高也不能过低。因为训练神经网络系统的过程，就是透过不断的迭代，找到让系统输出误差最小的参数的过程。每一次迭代都经过反向传播进行梯度下降，然而误差空间不是一个滑梯，一降到底，常规情况下就像坑洼的山地。

  学习率太小，那就很容易陷入局部最优，就是你认为的最低点并不是整个空间的最低点。

  如果学习率太高，那系统可能难以收敛，会在一个地方上蹿下跳，无法对准目标（目标是指误差空间的最低点），可以看图：

  ![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHfvnOjUK9rs3ls03k6qqWpLXE8Aia7elUjeQsicm20UBial9wicBUwJzPRw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

  xy轴是权值w平面，z轴是输出总误差。整个误差曲面可以看到两个明显的低点，显然右边最低，属于全局最优。而左边的是次低，从局部范围看，属于局部最优。而图中，在给定初始点的情况下，标出的两条抵达低点的路线，已经是很理想情况的***梯度下降路径***。

- 下面演示更新参数$w_5$的过程。设学习率为$\alpha$，$w_5^+$是$w_5$被更新后的值。则更新$w_5$的过程是：
  $$
  \begin{align}
  w_5^+ 
  &=w_5-\alpha\cdot\frac{\partial E_总}{\partial w_5}\qquad(5.16)\\
  &=0.5-0.5\cdot0.0818667051\\
  &=0.45906664745
  \end{align}
  $$
  更新$w_5$的函数的原理是：将总误差对的$w_5$偏导数视为误差对$w_5$的影响程度，而直接以该影响程度来表示偏离程度的大小：若是正偏离则这个偏导数值将为正数，因而应该减去这个偏离程度；反之则应该加上。因此用负号是较为合适的。而学习率为$\alpha$则可以削减该偏离程度的认知值。

  同理可以计算$w_6$、$w_7$、$w_8$的更新值，不再列出。

  根据式(5.13)及(5.16)，可以归纳出$w_i$的更新公式：
  $$
  \begin{align}
  w_i^+ 
  &=w_i-\alpha\cdot\frac{\partial E_总}{\partial w_i}&(5.16)\\
  &=w_i-\alpha\cdot\sigma_{O_1}\cdot{\rm out}_{h_1}\\
  &=w_i+\alpha\cdot[(o_1-{\rm out}_{O_1})\cdot {\rm out}_{O_1}\cdot(1-{\rm out}_{O_1})]\cdot{\rm out}_{h_1}&(5.17)
  \end{align}
  $$
  而对于$b$也可以透过将总误差对$b$的偏导数视为误差对b的影响程度来计算（略去步骤）。同理得：
  $$
  \begin{align}
  b_i^+ 
  &=b_i-\alpha\cdot\frac{\partial E_总}{\partial b_i}&(5.18)\\
  &=b_i-\alpha\cdot\sigma_{O_1}\\
  &=b_i+\alpha\cdot[(o_1-{\rm out}_{O_1})\cdot {\rm out}_{O_1}\cdot(1-{\rm out}_{O_1})]&(5.19)
  \end{align}
  $$

##### c) 输入层<-->隐藏层的权值及偏置b更新

- 以更新$w_1$为例：
  仔细观察，我们在求$w_5$的更新时，误差反向传递只能沿着输出层—>隐藏层的路径，即out($O_1$)—>in($O_1$)—>$w_5$，总误差只有一根线能传回来。但是求$w_1$时，误差反向传递路径是隐藏层—>输入层，但是隐藏层的神经元是有2根“神经纤维”的（$w_5$和$w_7$），所以总误差沿着2个路径回来。
  
  也就是说，计算$w_1$、$w_2$、$w_3$、$w_4$这些位于输入层和隐藏层之间的参数的偏导时，要分开来算。看图：
  
  ![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHLs3XKsuShFO0XqNELQYBaiaKxGCrtxMpbafMGz3AjW38K2UgxUL4j7w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
  
- 现在开始算总误差对$w_1$的偏导。按照式(5.12)依葫芦画瓢得
$$
  \begin{align}
\frac{\partial E_总}{\partial w_1} 
  &=
\frac{\partial E_总}{\partial {\rm out}_{h_1}}
  \cdot
\frac{\partial {\rm out}_{h_1}}{\partial {\rm in}_{h_1}}
  \cdot
\frac{\partial {\rm in}_{h_1}}{\partial w_1}&(5.20)
  \\

  &=
(\frac{\partial E_{O_1}}{\partial {\rm out}_{h_1}}+
  \frac{\partial E_{O_2}}{\partial {\rm out}_{h_1}})
\cdot
  \frac{\partial {\rm out}_{h_1}}{\partial {\rm in}_{h_1}}
  \cdot
  \frac{\partial {\rm in}_{h_1}}{\partial w_1}&(5.21)\\
  \end{align}
$$
  (5.20)有3个项。

  先算第1项。又拿出链式求导法则。其中该项的“+”号分左右两边：

  左边：
$$
  \begin{align}
  \frac{\partial E_{O_1}}{\partial {\rm out}_{h_1}}&=
  \frac{\partial E_{O_1}}{\partial {\rm in}_{O_1}}\cdot
  \frac{\partial {\rm in}_{O_1}}{\partial {\rm out}_{h_1}}\qquad(5.23)
  \end{align}
$$
  (5.23)的“乘”号左边：
$$
  \begin{align}
  \frac{\partial E_{O_1}}{\partial {\rm in}_{O_1}}
  &=
  \frac{\partial E_{O_1}}{\partial {\rm out}_{O_1}}\cdot
  \frac{\partial {\rm out}_{O_1}}{\partial {\rm in}_{O_1}}&(5.24)\\
  
  &=\frac{\partial (\frac{1}{2}(o_1-{\rm out}_{O_1}))^2}{\partial {\rm out}_{O_1}}\cdot
  \frac{\partial {\rm out}_{O_1}}{\partial {\rm in}_{O_1}}\\
  &=-(o_1-{\rm out}_{O_1})\cdot
  \frac{\partial {\rm out}_{O_1}}{\partial {\rm in}_{O_1}}\\
  &=0.7987314002\times0.1607595505\\
  &=0.1284037009
  
  \end{align}
$$
  (5.23)的“乘”号右边：
$$
  {\rm in}_{O_1}=w_5\cdot {\rm out}_{h_1}+w_6\cdot {\rm out}_{h_2}+1\cdot b_3\qquad(5.10)
$$

$$
  \begin{align}
  \frac{\partial {\rm in}_{O_1}}{\partial {\rm out}_{h_1}}&=
  \frac{\partial (w_5\cdot {\rm out}_{h_1}+w_6\cdot {\rm out}_{h_2}+1\cdot b_3)}
  {\partial {\rm out}_{h_1}}\\
  
  &=w_5\cdot{\rm out}_{h_1}^{(1-1)}+0+0\\
  &=w_5\qquad(5.25)\\
  &=0.5
  \end{align}
$$

  故(5.21)第1项的左边：
$$
  \begin{align}
  \frac{\partial E_{O_1}}{\partial {\rm out}_{h_1}}&=
  \frac{\partial E_{O_1}}{\partial {\rm in}_{O_1}}\cdot
  \frac{\partial {\rm in}_{O_1}}{\partial {\rm out}_{h_1}}\qquad(5.26)\\
  &=(5.24式)*(5.25式)\\
  &=0.1284037009*0.5\\
  &=0.06420185045
  \end{align}
$$
  同理(5.21)第1项的右边：
$$
  \begin{align}
  \frac{\partial E_{O_2}}{\partial {\rm out}_{h_1}}&=
  \frac{\partial E_{O_2}}{\partial {\rm in}_{O_2}}\cdot
  \frac{\partial {\rm in}_{O_2}}{\partial {\rm out}_{h_1}}\\
  &=[-(o_2-{\rm out}_{O_2})\cdot
  \frac{\partial {\rm out}_{O_2}}{\partial {\rm in}_{O_2}}]\cdot
  \frac{\partial {\rm in}_{O_2}}{\partial {\rm out}_{h_1}}
  \\
  &=-(o_2-{\rm out}_{O_2})\cdot
  [{\rm out}_{O_2}(1-{\rm out}_{O_2})]\cdot{w_7}
  \\
  &=-0.0145365614
  \end{align}
$$
  所以(5.21)第1项值为：
$$
  \begin{align}
  \frac{\partial E_总}{\partial {\rm out}_{h_1}}
  &=
  \frac{\partial E_{O_1}}{\partial {\rm out}_{h_1}}+
  \frac{\partial E_{O_2}}{\partial {\rm out}_{h_1}}
  \\
  &=0.06420185045+(-0.0145365614)
  \\
  &=0.04966528905
  \end{align}
$$
  然后算(5.20)的第二项：激活函数也是Sigmoid函数，求导有
$$
  \begin{align}
  \frac{\partial {\rm out}_{h_1}}{\partial {\rm in}_{h_1}}
  &=
  {\rm out}_{h_1}(1-{\rm out}_{h_1})
  \\
  &=
  0.6456563062\times(1-0.6456563062)\\
  &=0.2287842405
  \end{align}
$$
  然后算(5.20)的第三项：
$$
  \begin{align}
  \frac{\partial {\rm in}_{h_1}}{\partial w_1}
  &=
  \frac{\partial (w_1\cdot i_1+w_2\cdot i_2+1\cdot b_1)}{\partial w_1}
  \\
  &=
  w_1^{(1-1)}i_1+0+0\\
  &=i_1=0.1
  \end{align}
$$
  三项相乘，得(5.20)得值
$$
  \begin{align}
  \frac{\partial E_总}{\partial w_1} 
  &=
  \frac{\partial E_总}{\partial {\rm out}_{h_1}}
  \cdot
  \frac{\partial {\rm out}_{h_1}}{\partial {\rm in}_{h_1}}
  \cdot
  \frac{\partial {\rm in}_{h_1}}{\partial w_1}&(5.20)
  \\
  &=0.0011362635
  \end{align}
$$
  至此，总误差对$w_1$的偏导的值已经求出。下面对上面的计算进行一般化：

  ![image-20200425180226479](/Users/phantef/Library/Application Support/typora-user-images/image-20200425180226479.png)

- 由前述的一般化结论，可以更新$w_1$的值（取学习率0.1）：
  $$
  \begin{align}
  w_1^+ 
  &=w_1-\alpha\cdot\frac{\partial E_总}{\partial w_1}\qquad(5.29)\\
  &=0.1-0.1\cdot0.0011362635\\
  &=0.0998863737
  \end{align}
  $$
  将$w_i$ (i = 隐藏层的i)的更新公式归纳为一般结论：
  $$
  \begin{align}
  w_i^{+(h)} 
  &=w_i^{(h)}-\alpha\cdot\frac{\partial E_总}{\partial w_i^{(h)}}\\
  &=w_i^{(h)}-\alpha\cdot(-\sum_{O}\sigma_Ow_O)\cdot
  {\rm out}_i^{(h)}(1-{\rm out}_i^{(h)})\cdot i\qquad(5.30)\\
  \end{align}
  $$
  其中
  $$
  \sum_{O}\sigma_Ow_O=\sum_{O}
  {\frac{\partial E_O}{\partial {\rm out}_O}
  \cdot
  \frac{\partial {\rm out}_O}{\partial {\rm in}_O}
  \cdot
  \frac{\partial {\rm in}_O}{\partial {\rm out}_h}}
  $$
  其中的所有“O”和“h”其实应该规范地写为
  $$
  {O_i^{(h)}}, h_i
  $$
  代表该O是泛指该隐藏层“$h_i$”下一层会连接到的那些输出层的神经元，而因此上式应该被详细地写为：
  $$
  \sum_{O_i^{(h)}}\sigma_{O_i^{(h)}}w_{O_i^{(h)}}=\sum_{O_i^{(h)}}
  {\frac{\partial E_{O_i^{(h)}}}{\partial {\rm out}_{O_i^{(h)}}}
  \cdot
  \frac{\partial {\rm out}_{O_i^{(h)}}}{\partial {\rm in}_{O_i^{(h)}}}
  \cdot
  \frac{\partial {\rm in}_{O_i^{(h)}}}{\partial {\rm out}_{h_i}}}
  $$
  从而得到了输入层<-->隐藏层的$w_i$更新公式。

### **4. 结论：**

我们透过亲力亲为的计算，走过了正向传播，也体会了反向传播，完成了一次训练（**迭代**）。同时多次计算发现，随着迭代加深，输出层的误差会越来越小，专业点就是系统趋于收敛。来一张系统误差随迭代次数变化的图来表明这个描述：

![image-20200425122500830](/Users/phantef/Library/Application Support/typora-user-images/image-20200425122500830.png)

## ▌六、沙漠的绿洲：代码实现

### **1. 代码代码！**

其实已经有很多机器学习的框架可以很简单的实现神经网络。但是我们的目标是：在看懂算法之后，我们是否能照着算法的整个过程，去实现一遍，可以加深对算法原理的理解，以及对算法实现思路的的理解。顺便说打个call，numpy这个库，你值得拥有！

- 代码实现如下。代码里已经做了尽量啰嗦的注释，关键实现的地方对标了公式的编号，如果看的不明白的地方多回来啃一下算法推导。对应代码也传到了github上。
- 代码能自己定义神经网络的结构，支持深度网络。代码实现了对红蓝颜色的点做分类的模型训练，通过3层网络结构，改变隐藏层的神经元个数，通过图形显示隐藏层神经元数量对问题的解释能力。
- 代码中还实现了不同激活函数。隐藏层可以根据需要换着激活函数玩，输出层一般就用sigmoid，当然想换也随你喜欢～

【代码详见bp.py】

### **2. 晒图晒图！**

关于误差曲线(这里只举其中一个栗子)：

- 通过看误差曲线，可以从一定程度上判定网络的效果，模型训练是否能收敛，收敛程度如何，都可以从误差曲线对梯度下降的过程能见一二。

  ![image-20200425122856926](/Users/phantef/Library/Application Support/typora-user-images/image-20200425122856926.png)

3层网络的结构下，隐藏层只有一层，看图说明一下隐藏层神经元个数变化对神经网络表达能力的影响：

- 当隐藏层只有1个神经元时：就像文章刚开始说的，一个神经元，就是个线性分类器，表达能力就只有***一条直线***而已，见式（3.6）（上左图）

- 2个神经元：线开始有点弯曲了，但是这次结果一点都不明显，尴尬。但从原理上神经网络开始具备了非线性表达能力（上右图）
- 随着隐藏层神经元个数不断增加，神经网络表达能力越来越强，分类的效果越来越好。当然也不是神经元越多越好，可以开始考虑深度网络是不是效果更好一些。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjzr3IibYwltdavich3yf8sbJHI0YDPFrTOAB5N9ibCEbvRK3YLia46DZ8HqVfQz7Fj303qUSCWWkK5Hlw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

可以参考这个网站提供的可视化训练BP网络过程加深印象： http://www.emergentmind.com/neural-network

## ▌七、没有结局

记住一点，bp神经网络是其他各种神经网络中最简单的一种。只有学会了它，才能以此为基础展开对其他更复杂的神经网络的学习。


虽然推导了并实现了算法，但是仍然是有很多疑问，这里就作为抛砖引玉吧：

- 神经网络的结构，即几层网络，输入输出怎么设计才最有效？
- 数学理论证明，三层的神经网络就能够以任意精度逼近任何非线性连续函数。那么为什么还需要有深度网络？
- 在不同应用场合下，激活函数怎么选择？
- 学习率怎么怎么选择？
- 训练次数设定多少训练出的模型效果更好？

AI，从入门到放弃，首篇结束。

参考博客：

> https://zhuanlan.zhihu.com/p/38006693
>
> https://www.cnblogs.com/charlotte77/p/5629865.html
